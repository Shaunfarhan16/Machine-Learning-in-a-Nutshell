{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94759c3f-f05a-4939-8701-2da2bb9cca52",
   "metadata": {},
   "source": [
    "# **Machine Learning For Beginners**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4e67d-53ec-465a-be35-a9dee275a1e2",
   "metadata": {},
   "source": [
    "## 1. Foundations You Actually Need\n",
    "\n",
    "| Goal                                           | What you’ll cover                                                                                      | Resources                                                 |\n",
    "| ---------------------------------------------- | ------------------------------------------------------------------------------------------------------ | --------------------------------------------------------- |\n",
    "| **Python for Data**                            | Installing Anaconda / Miniconda, Jupyter, VS Code; the *pandas* & *NumPy* 101 you really need          | Official tutorials + my cheat-sheet                       |\n",
    "| **Math Refresher**                             | Vectors & matrices (linear algebra), derivatives (very light calculus), basic probability & statistics | 3Blue1Brown “Essence of LA”, Khan Academy micro-playlists |\n",
    "| **Version Control** (optional but recommended) | Git basics, GitHub repo setup for your projects                                                        | Try Git interactive tutorial                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b19e96-4ad3-4ab2-bcb8-f650d06bf808",
   "metadata": {},
   "source": [
    "## 2. Core Machine-Learning Concepts\n",
    "\n",
    "What counts as “learning”? datasets, features vs labels\n",
    "\n",
    "Train / validation / test splits, random vs time-series splitting\n",
    "\n",
    "Loss functions and why optimization ≠ memorisation\n",
    "\n",
    "Bias–variance trade-off (your first mental model of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2a546-886d-4219-a6f6-f6830163dbf3",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning — Regression & Classification\n",
    "\n",
    "| Algorithm                       | Why it’s useful                          | Key hyper-params            | scikit-learn class                       |\n",
    "| ------------------------------- | ---------------------------------------- | --------------------------- | ---------------------------------------- |\n",
    "| Linear & Logistic Regression    | Gold-standard baselines                  | regularisation strength     | `LinearRegression`, `LogisticRegression` |\n",
    "| k-Nearest Neighbours            | Intuitive, non-parametric                | `n_neighbors`               | `KNeighborsClassifier`/`Regressor`       |\n",
    "| Decision Trees & Random Forests | Handle non-linear relations, little prep | `max_depth`, `n_estimators` | `DecisionTree*`, `RandomForest*`         |\n",
    "| Gradient Boosting / XGBoost     | State-of-the-art tabular                 | learning rate, trees        | `XGBClassifier`/`Regressor`              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efb435-67fa-4403-8e58-bffca4e0df5a",
   "metadata": {},
   "source": [
    "## 4. Unsupervised Learning\n",
    "\n",
    "Clustering: k-Means, DBSCAN, Agglomerative\n",
    "\n",
    "Dimensionality Reduction: PCA, t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c587fe2-7601-4625-a31d-e98802f6365f",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Tuning\n",
    "\n",
    "Metrics cheat-sheet (RMSE, MAE, Accuracy, Precision-Recall, ROC-AUC)\n",
    "\n",
    "Cross-validation & nested CV\n",
    "\n",
    "Hyper-parameter search (Grid, Random, Bayesian)\n",
    "\n",
    "Intro to pipelines so you stop leaking data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb2be9-f5dc-4ea4-905f-e48f805c7904",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering & Data Wrangling\n",
    "\n",
    "Handling missing data, text, dates, categories\n",
    "\n",
    "Encoding schemes (one-hot, ordinal, target, embeddings)\n",
    "\n",
    "Feature selection & importance (permutation, SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db227833-2232-4d19-8ac3-6bb105b14efa",
   "metadata": {},
   "source": [
    "## 7. Neural Networks & Deep Learning (Starter Pack)\n",
    "\n",
    "How a perceptron becomes a deep net\n",
    "\n",
    "Popular architectures: MLP → CNN → RNN/Transformer basics\n",
    "\n",
    "Framework tour: Keras/TensorFlow vs PyTorch (we’ll pick one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7f194-b9f0-4cfd-9104-c56013dde870",
   "metadata": {},
   "source": [
    "## 8. Model Deployment & MLOps Basics\n",
    "\n",
    "Saving models (joblib, pickle, ONNX)\n",
    "\n",
    "Serving with FastAPI or Flask (containerised via Docker)\n",
    "\n",
    "Monitoring drift & performance in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ef2a6-bc34-44ae-9aa5-363cfdb5dd75",
   "metadata": {},
   "source": [
    "## 9. Ethics, Fairness & Interpretability\n",
    "\n",
    "Data bias, fairness metrics, responsible AI guidelines\n",
    "\n",
    "Explainers: LIME, SHAP\n",
    "\n",
    "Privacy (GDPR basics, differential privacy intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff378af-6adf-4420-969a-eb4459f8ad62",
   "metadata": {},
   "source": [
    "### Applying Models to Real Files — Your “Universal Pattern”\n",
    "\n",
    "Below is the repeatable recipe we’ll use again and again (pseudocode now; we’ll flesh out real examples when you’re ready to run them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6f870-4ee0-4a8b-9e07-d5c835fd4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load\n",
    "df = pd.read_csv(\"your_data.csv\")          # or read_excel, read_json, read_sql...\n",
    "\n",
    "# 2. Define X & y\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# 3. Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# 4. Preprocess\n",
    "num_cols = X.select_dtypes(include=\"number\").columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# 5. Model\n",
    "model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "# 6. Pipeline = preprocess + model\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate\n",
    "preds = pipe.predict(X_val)\n",
    "print(classification_report(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded7c16-73d9-4c8b-812f-e74af1a307b8",
   "metadata": {},
   "source": [
    "# Module 1 – Set-up & “Python for Data” from Absolute Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce7733-f003-4000-ade1-f7388cfdf711",
   "metadata": {},
   "source": [
    "### 1. Pick & install your Python distribution\n",
    "\n",
    "| Option                       | Best for                                                     | Quick steps                                                                                                                                                                                                                |\n",
    "| ---------------------------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Anaconda (≈ 3 GB)**        | You want “batteries-included” and a GUI launcher             | 1. Download the **Anaconda 3** installer for your OS from [https://www.anaconda.com/download](https://www.anaconda.com/download). <br>2. Run the installer → tick *Add Anaconda to PATH* if offered (safe on Windows 10+). |\n",
    "| **Miniconda (≈ 80 MB)**      | You prefer lightweight installs & only the packages you need | 1. Grab **Miniconda** from [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html). <br>2. Run installer → accept defaults.                                                       |\n",
    "| **System Python + pip/venv** | You already use Homebrew / apt / Chocolatey                  | 1. Install Python 3.10 + via your package manager. <br>2. `python -m venv ml-env && source ml-env/bin/activate` (Linux/macOS) or `.\\ml-env\\Scripts\\activate` (Win).                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d356d5c-0d4d-4d06-abdc-821e521c0d02",
   "metadata": {},
   "source": [
    "### 2. Create a clean ML environment (conda users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b766a20-9c23-405c-8b42-b08fd2cff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Terminal (macOS/Linux) or Anaconda Prompt / PowerShell (Windows)\n",
    "conda update -n base -c defaults conda             # keep conda itself current\n",
    "conda create -n ml101 python=3.11 numpy pandas jupyterlab matplotlib seaborn scikit-learn\n",
    "conda activate ml101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96419f9-3d49-4136-ab17-cfb0142fbf31",
   "metadata": {},
   "source": [
    "(If you went with venv/pip, just run pip install numpy pandas jupyterlab matplotlib seaborn scikit-learn inside the virtual-env.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e4df0-1dbd-4db8-8403-4f47b9a9d3f4",
   "metadata": {},
   "source": [
    "### 3. Launch JupyterLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42223903-2ec4-4c3c-80b8-5d9874cf0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter lab / python notebook / python -m notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e248de2-fdc9-48e6-8dae-b6a2cd51bab9",
   "metadata": {},
   "source": [
    "### 4. Your first 15 lines of “data Python”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a62ee-4878-49eb-abac-c30e7c98be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1-liner: download a tiny CSV (UK car prices sample) straight from GitHub\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv\"\n",
    "cars = pd.read_csv(url)\n",
    "\n",
    "cars.head()          # show first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4e180-eb99-48b2-bd7a-0e917f471b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.shape           # rows, columns\n",
    "cars.describe()      # numeric summary stats\n",
    "cars['origin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f67b1e-7202-4760-8055-c5ffe365d01a",
   "metadata": {},
   "source": [
    "### 5. Mini-exercise (15 min)\n",
    "\n",
    "Pick a dataset you care about (CSV, Excel, JSON… anything tabular).\n",
    "• Example sources: Kaggle “Titanic”, UCI Machine Learning Repository, your own spreadsheets.\n",
    "\n",
    "Load it into a new notebook just like we did with cars.\n",
    "\n",
    "Answer three basic questions about it, e.g.\n",
    "How many rows? • Which columns are numeric vs. text? • What’s the mean of one interesting column?\n",
    "\n",
    "(Optional but recommended) Push the notebook to a GitHub repo called ml-learning-journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c691c-2ffc-4794-a204-7e7e99c0de58",
   "metadata": {},
   "source": [
    "# Module 2 – Core Machine-Learning Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8277d-8ee2-47df-a37f-86576699ee31",
   "metadata": {},
   "source": [
    "### 1. What “learning” means in ML\n",
    "\n",
    "- Data = rows of examples. Each row is called an instance / sample / observation.\n",
    "\n",
    "- Features (X) are the input columns we give to the algorithm.\n",
    "\n",
    "- Target / label (y) is the output column we want it to predict.\n",
    "\n",
    "- Supervised learning means we already know y for past data and train a model to map X → y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aed99f-be22-4007-91b8-cebe3c28655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars.drop('mpg', axis=1)   # features\n",
    "y = cars['mpg']                # target\n",
    "print(X.columns.tolist()[:5], '...', len(X.columns), 'features total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fdb8e-75da-4791-958b-c14b36b17c14",
   "metadata": {},
   "source": [
    "### 2. Train / Validation / Test split (why & how)\n",
    "\n",
    "| Split          | Size (rule-of-thumb) | Purpose                               | Kept hidden from…                      |\n",
    "| -------------- | -------------------- | ------------------------------------- | -------------------------------------- |\n",
    "| **Train**      | 60-80 %              | Fit the model                         | Nobody                                 |\n",
    "| **Validation** | 10-20 %              | Tune hyper-parameters, early stopping | The model (during fit)                 |\n",
    "| **Test**       | 10-20 %              | Final, unbiased report                | You, until *everything* else is frozen |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47886082-b716-4e2a-b1bf-c15276079d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42)        # 70 % train\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42)  # 15 % + 15 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a455c05-d9cf-46b8-831b-700efa61d399",
   "metadata": {},
   "source": [
    "Why random_state? It lets others reproduce your exact split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c3589-8cba-412a-af94-64778f4cfcaf",
   "metadata": {},
   "source": [
    "### 3. Loss functions ≠ evaluation metrics\n",
    "\n",
    "| Task                  | Typical **loss** (optimised during training) | Typical **metric** (reported to humans) |\n",
    "| --------------------- | -------------------------------------------- | --------------------------------------- |\n",
    "| Regression            | Mean-Squared-Error (MSE)                     | RMSE, MAE, R²                           |\n",
    "| Binary classification | Log-loss (a.k.a. cross-entropy)              | Accuracy, F1, ROC-AUC                   |\n",
    "| Multiclass            | Categorical cross-entropy                    | Accuracy, macro-F1                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adb4d0-7889-4eb9-adeb-eb1af8f2816d",
   "metadata": {},
   "source": [
    "**Rule: the algorithm never sees your metric; it only minimises loss.**\n",
    "\n",
    "That’s why you occasionally get models that score great on loss but mediocre on your business KPI—choose the right metric and monitor both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8262dd8-019e-4b17-ba3f-250158eb359d",
   "metadata": {},
   "source": [
    "### 4. Bias–Variance Trade-off (your first mental model)\n",
    "\n",
    "Imagine fitting dots with a line vs a wiggly curve:\n",
    "\n",
    "- High bias = model too simple → under-fits, large error on both train & val.\n",
    "\n",
    "- High variance = model too complex → memorises train set, but val error explodes.\n",
    "\n",
    "Goal: sit in the “Goldilocks” zone where train error is low and the gap to val error is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc6eb1-3087-4a49-8340-930d91108639",
   "metadata": {},
   "source": [
    "### 5. Mini-project – “Hello, Overfitting!”\n",
    "\n",
    "Dataset: the same cars CSV.\n",
    "\n",
    "Objective: Predict mpg (fuel efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afcd9a-2bb4-469a-bfd8-33394410bf5f",
   "metadata": {},
   "source": [
    "### **Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6bbe3b-66ec-4ae0-bd1d-2c6479656eed",
   "metadata": {},
   "source": [
    "#### Quick-and-dirty numeric-only subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c870323-4764-4758-b599-8879c0ae1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = cars.select_dtypes('number').columns\n",
    "X_num = cars[numeric_cols].drop('mpg', axis=1)\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527e225-7dbb-410a-8d4c-9c52e055abf4",
   "metadata": {},
   "source": [
    "#### Create three models of increasing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13d442-c2bf-4bfa-bcea-737f50c503ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Tree_depth2\": DecisionTreeRegressor(max_depth=2, random_state=0),\n",
    "    \"RandomForest_100\": RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd200b-2c41-40e7-9dae-fef162d8f259",
   "metadata": {},
   "source": [
    "#### Train/val split (80 % / 20 %)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45077c43-9c70-43c8-b1b7-b91566a45c3a",
   "metadata": {},
   "source": [
    "#### Fit & print RMSE on both splits for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131410-5912-4d0b-b3b7-3e2ac3ffa8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, m.predict(X_train)))\n",
    "    rmse_val   = np.sqrt(mean_squared_error(y_val,   m.predict(X_val)))\n",
    "    print(f\"{name:15}  RMSE train: {rmse_train:5.2f}  val: {rmse_val:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29af696-7841-46dd-b014-b8efd79257b7",
   "metadata": {},
   "source": [
    "#### Interpret\n",
    "\n",
    "Which model is high-bias? (look for big errors everywhere)\n",
    "\n",
    "Which is high-variance? (tiny train error, much worse val error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0da24-5414-448a-9380-29f632565b82",
   "metadata": {},
   "source": [
    "### 6. (Quick) Metrics cheat-sheet\n",
    "\n",
    "**Regression**\n",
    "\n",
    "- RMSE = √MSE → penalises large errors\n",
    "\n",
    "- MAE = mean |error| → more robust to outliers\n",
    "\n",
    "- R² = 1 − (SS_res / SS_tot) → fraction of variance explained\n",
    "\n",
    "**Classification**\n",
    "\n",
    "- Accuracy = (TP+TN)/all\n",
    "\n",
    "- Precision = TP/(TP+FP) (“When I say spam, how often am I right?”)\n",
    "\n",
    "- Recall = TP/(TP+FN) (“How many actual spams did I catch?”)\n",
    "\n",
    "- F1 = harmonic mean of precision & recall (balance)\n",
    "\n",
    "- ROC-AUC = probability a random positive ranks above a random negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc794c16-080f-407e-bc55-9ec261e2a4f9",
   "metadata": {},
   "source": [
    "### 7. Stretch: Cross-validation in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b24eb4-21dc-46fb-829f-224c8452678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=0)\n",
    "rmse_scores = -cross_val_score(gb, X_num, y,\n",
    "                               cv=5,       # 5-fold\n",
    "                               scoring='neg_root_mean_squared_error')\n",
    "print(\"5-fold RMSE:\", rmse_scores.round(2), \"mean\", rmse_scores.mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fa2b8-f56d-47c3-8041-8387b984c5a4",
   "metadata": {},
   "source": [
    "### 8. You’re ready for Module 3\n",
    "\n",
    "Once you’ve:\n",
    "\n",
    "✔️ split data,\n",
    "\n",
    "✔️ observed bias/variance,\n",
    "\n",
    "✔️ calculated at least one metric on train & val,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d64619-3d45-493c-853b-65f71c070643",
   "metadata": {},
   "source": [
    "# Module 3 – First Real Models: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05599db-8540-4011-a64e-31a4f40edafa",
   "metadata": {},
   "source": [
    "### 1. Why start with Linear & Logistic Regression?\n",
    "\n",
    "They’re the simplest learners that already illustrate training ⇢ evaluation ⇢ interpretation. Everything you meet later (trees, boosting, neural nets) is just a fancier way to:\n",
    "\n",
    "Define a score to minimise (loss)\n",
    "\n",
    "Search parameter space for the minimum\n",
    "\n",
    "Generalise to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d8366-00af-4dd7-86d9-03391ad14583",
   "metadata": {},
   "source": [
    "### 2. Linear Regression (predicting numbers)\n",
    "\n",
    "| Idea            | One-sentence reminder                                              | In `scikit-learn`    |\n",
    "| --------------- | ------------------------------------------------------------------ | -------------------- |\n",
    "| **Model**       | ŷ = β₀ + β₁x₁ + … + βₖxₖ                                           | `LinearRegression()` |\n",
    "| **Loss**        | Minimise Mean-Squared-Error                                        | built-in             |\n",
    "| **Assumptions** | roughly linear relation, independent errors, low multicollinearity | check residual plots |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ec73f-0572-4933-bb6a-0f6fc1d2e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_num, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, lin.predict(X_val)))\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# peek at coefficients\n",
    "for name, coef in zip(X_num.columns, lin.coef_):\n",
    "    print(f\"{name:15s} {coef:8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838d777-9ac7-40ad-87fe-d937cc00e727",
   "metadata": {},
   "source": [
    "Exercise: plot predicted vs. actual; perfect predictions would lie on the 45° line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcd6b9-72f1-4026-b5ef-9152f9c751ef",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression (predicting classes)\n",
    "\n",
    "Same math; just squashes output through the sigmoid to give probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ee04d-73bf-4f2c-bef1-e92158a196b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "logit = LogisticRegression(max_iter=1000)      # default solver = LBFGS\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362d6ca-3d15-40b9-91a9-f3e5757b4515",
   "metadata": {},
   "source": [
    "Key hyper-parameters\n",
    "\n",
    "penalty = \"l2\" (ridge) or \"l1\" (lasso)\n",
    "\n",
    "C = inverse regularisation strength (smaller = stronger penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b9de7-56ab-474c-bb73-96350cfe4b33",
   "metadata": {},
   "source": [
    "### 4. Three Non-parametric Baselines\n",
    "\n",
    "| Algorithm                | When it shines                             | `sklearn` class                  | Must-tune                                   |\n",
    "| ------------------------ | ------------------------------------------ | -------------------------------- | ------------------------------------------- |\n",
    "| **k-Nearest-Neighbours** | data is small, decision boundary irregular | `KNeighborsClassifier/Regressor` | `n_neighbors`, distance metric              |\n",
    "| **Decision Tree**        | easy interpretability, handles mixed types | `DecisionTree*`                  | `max_depth`, `min_samples_leaf`             |\n",
    "| **Random Forest**        | strong default on tabular data             | `RandomForest*`                  | `n_estimators`, `max_depth`, `max_features` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f14a6b-8306-47ad-ae33-7994620f348f",
   "metadata": {},
   "source": [
    "**Scaling note** – KNN needs scaled features; trees/forests do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcbe1c-ad00-40ce-a1e8-eef1280a4101",
   "metadata": {},
   "source": [
    "### 5 Gradient Boosting / XGBoost\n",
    "\n",
    "Ensemble of shallow trees trained sequentially; each new tree fixes the predecessor’s errors.\n",
    "    \n",
    "Why you care: wins most Kaggle comps; handles missing values; robust to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f6356-d4f6-4187-85e8-f925bd466186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "        n_estimators=300,       # trees\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f79f20-06b3-4140-9ffd-0eb5f64fbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tune **learning_rate ↔ n_estimators** together (small LR needs more trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bdd21-23bd-4659-af96-3ddc2c672015",
   "metadata": {},
   "source": [
    "### 6. End-to-End Example: Titanic Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342448-b5b3-409e-8634-c6a27eb59abb",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4fc9a-0595-400c-9980-a1e5247ae237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, pandas as pd\n",
    "df = sns.load_dataset(\"titanic\")     # one-liner; or use Kaggle CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812f029-159b-4657-967c-7bae05f8e7c0",
   "metadata": {},
   "source": [
    "#### Define target & basic preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b31bb-1c5d-44fa-ac8a-0854c56be289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "y = df[\"survived\"]\n",
    "X = df.drop(columns=[\"survived\"])\n",
    "\n",
    "num_cols = X.select_dtypes(include=\"number\").columns\n",
    "cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, num_cols),\n",
    "    (\"cat\", categorical_pipe, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c64448-1bd5-4df3-9c0b-c6c03a97e651",
   "metadata": {},
   "source": [
    "#### Swap in any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f80546-28e0-49f4-a7d0-bdeef65051be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic\":   LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    \"XGB\":        XGBClassifier(n_estimators=300, learning_rate=0.05,\n",
    "                                max_depth=4, subsample=0.7,\n",
    "                                colsample_bytree=0.8, random_state=0)\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2,\n",
    "                                            stratify=y, random_state=0)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    acc = pipe.score(X_val, y_val)\n",
    "    print(f\"{name:12s} accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd83b6-dea5-4c8a-ab13-2774a5aee15b",
   "metadata": {},
   "source": [
    "#### Interpret results\n",
    "\n",
    "Use classification_report for precision/recall; for tree-based models print feature_importances_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac91fc-8862-412c-a349-c02b8f3875da",
   "metadata": {},
   "source": [
    "### 7. Mini-project (your turn)\n",
    "\n",
    "Goal: build & compare at least two models on a CSV you choose (ideas: loan default, heart-disease, customer churn).\n",
    "\n",
    "**Checklist**\n",
    "\n",
    "- Load CSV into df\n",
    "\n",
    "- Identify target column and split features/label\n",
    "\n",
    "- Build preprocess with numeric+categorical pipelines\n",
    "\n",
    "- Train\n",
    "\n",
    "(a) Logistic Regression (baseline)\n",
    "\n",
    "(b) Random Forest or XGBoost (stronger)\n",
    "\n",
    "- Report accuracy + confusion matrix + classification report\n",
    "\n",
    "- Briefly explain which features matter most (coef or importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e984f-5bce-4e67-9167-0dd938679759",
   "metadata": {},
   "source": [
    "### 8. What’s Next\n",
    "\n",
    "Once you’re comfortable training, evaluating, and interpreting these first models we’ll move on to Module 4: Unsupervised Learning (clustering & dimensionality reduction) and tackle data without labels.\n",
    "\n",
    "Just ping me when:\n",
    "\n",
    "- you finish the mini-project (even partially), or\n",
    "\n",
    "- something breaks and you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242385bd-1b3d-44ed-aaf0-8598120f82e4",
   "metadata": {},
   "source": [
    "# Module 4 – Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc5ece-6de1-4e95-9457-22bee5770693",
   "metadata": {},
   "source": [
    "### 1. Why “unsupervised”?\n",
    "\n",
    "No target column y to guide the algorithm—just raw features X.\n",
    "\n",
    "You ask your model to discover structure (clusters, manifolds, directions of maximum variance).\n",
    "Typical use-cases: customer segmentation, anomaly detection, data exploration & visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e434ac-f1e3-412f-895b-035b913e31d3",
   "metadata": {},
   "source": [
    "### 2. Clustering Algorithms in One Glance\n",
    "\n",
    "| Algorithm                        | Intuition                                         | Pros                                           | Watch-outs                                            | `sklearn` class           | Must-tune                                         |\n",
    "| -------------------------------- | ------------------------------------------------- | ---------------------------------------------- | ----------------------------------------------------- | ------------------------- | ------------------------------------------------- |\n",
    "| **k-Means**                      | “Find *k* centroids, assign points to nearest.”   | Fast, scales to 10⁶ rows                       | Needs k; hates outliers & non-spherical shapes        | `KMeans`                  | `n_clusters`, `init`, `n_init`                    |\n",
    "| **DBSCAN**                       | “Core points in dense areas → expand clusters.”   | Detects arbitrary shapes, auto-finds #clusters | Sensitive to ε & minPts; struggles in varying density | `DBSCAN`                  | `eps`, `min_samples`                              |\n",
    "| **Agglomerative (Hierarchical)** | “Start as singletons, iteratively merge closest.” | Dendrogram = visual story; no k upfront        | O(n²) memory; large datasets heavy                    | `AgglomerativeClustering` | `n_clusters` *or* `distance_threshold`, `linkage` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26814d-da6b-4ba8-b998-dbdc50d2cf51",
   "metadata": {},
   "source": [
    "**Rule of thumb**\n",
    "\n",
    "If you know roughly how many segments you want, start with k-Means.\n",
    "\n",
    "If you suspect weird shapes / noise, try DBSCAN.\n",
    "\n",
    "If you need a hierarchy, go agglomerative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3ba82-7779-4b7f-8cbe-994d7fc57c52",
   "metadata": {},
   "source": [
    "#### 2.1 k-Means in Code (Iris example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc55af-2f8e-42e1-893d-aa6303fa01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "iris   = load_iris(as_frame=True)\n",
    "X      = iris.data\n",
    "scaler = StandardScaler()\n",
    "X_std  = scaler.fit_transform(X)\n",
    "\n",
    "km = KMeans(n_clusters=3, n_init='auto', random_state=42)\n",
    "labels = km.fit_predict(X_std)\n",
    "\n",
    "print(\"Silhouette:\", silhouette_score(X_std, labels).round(3))\n",
    "pd.crosstab(labels, iris.target)       # compare to true species (just for curiosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c0784-520b-4fd3-9429-861c63ec8f8a",
   "metadata": {},
   "source": [
    "### 3. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61688711-a97a-4386-8a7a-b933d0b64052",
   "metadata": {},
   "source": [
    "#### 3.1 PCA – the workhorse\n",
    "\n",
    "Goal: find orthogonal directions (principal components) that capture maximal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa24792-7dd2-4e88-bbde-95239a596f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d473f-2ee8-4dcc-89d4-5c139b5e0947",
   "metadata": {},
   "source": [
    "**Rule:** always standardise numeric data before PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecefcd-7537-4476-a00c-11f867ad4a44",
   "metadata": {},
   "source": [
    "#### 3.2 Visualise clusters after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710322a7-fd6a-47a0-a7b1-d7bace5d0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, s=30)\n",
    "plt.xlabel(\"PC 1\"); plt.ylabel(\"PC 2\"); plt.title(\"k-Means clusters on Iris\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93387e3e-73ab-4310-9194-e202262408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Don’t worry if PC 1+PC 2 < 80 % variance—visual clarity is what you want.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276026a-4292-4192-972c-b5f517c64693",
   "metadata": {},
   "source": [
    "#### 3.3 t-SNE & UMAP – for non-linear manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13ef45-2143-4cac-b7a8-a47b16359558",
   "metadata": {},
   "source": [
    "| Method    | Use when                                    | Notes                                                                                       |\n",
    "| --------- | ------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **t-SNE** | you only care about 2-D/3-D visual clusters | Great visuals, but slow > 10 k rows; distances **not** meaningful beyond nearest neighbours |\n",
    "| **UMAP**  | you want speed & can keep more dimensions   | Preserves global & local structure better; supports `n_components>3`                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed87dd-1648-4a64-8101-95e273fe48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=40, random_state=0)\n",
    "X_tsne = tsne.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce10efe-f0c2-419b-9d92-ba3f5ed722cf",
   "metadata": {},
   "source": [
    "### 4. Putting It All Together – Typical Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d69c7-5d54-4c83-a16c-8ae4a1d9ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load / clean\n",
    "df = pd.read_csv(\"your_data.csv\")\n",
    "X  = df.select_dtypes(include=\"number\")    # or mix in encoded categoricals\n",
    "\n",
    "# 1. Scale\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 2. Pick k with the silhouette / elbow method\n",
    "sil_scores = {}\n",
    "for k in range(2, 11):\n",
    "    lab = KMeans(n_clusters=k, n_init='auto', random_state=0).fit_predict(X_std)\n",
    "    sil_scores[k] = silhouette_score(X_std, lab)\n",
    "best_k = max(sil_scores, key=sil_scores.get)\n",
    "\n",
    "# 3. Final model\n",
    "kmeans = KMeans(n_clusters=best_k, n_init='auto', random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(X_std)\n",
    "\n",
    "# 4. Attach labels back\n",
    "df[\"cluster\"] = cluster_labels\n",
    "df.groupby(\"cluster\").mean().round(1)      # cluster profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf7de7-a77a-4db0-81aa-44e8d70221ff",
   "metadata": {},
   "source": [
    "### 5. Mini-Project – Customer Segmentation\n",
    "\n",
    "Dataset suggestion: “Mall Customers” (CSV, 200 rows, Age / Annual Income / Spending Score).\n",
    "\n",
    "Download from https://github.com/vincentarelbundock/Rdatasets/raw/master/csv/datasets/mall.csv\n",
    " or use your own e-commerce data.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- EDA – check missing values, basic stats.\n",
    "\n",
    "- Scale numeric columns.\n",
    "\n",
    "- Find k\n",
    "\n",
    "Plot elbow (inertia vs. k)\n",
    "\n",
    "Compute silhouette for k = 2…10.\n",
    "\n",
    "- Fit k-Means with the best k.\n",
    "\n",
    "- Visualise clusters in 2-D with PCA or t-SNE.\n",
    "\n",
    "- Profile clusters – mean age, income, spend; label groups (e.g. “High-income low-spend”).\n",
    "\n",
    "- (Optional) Try DBSCAN—does it split any cluster further or flag outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467419c6-30bb-4ffb-9d56-f8eee6cab640",
   "metadata": {},
   "source": [
    "### 6. Where This Fits in the Bigger Picture\n",
    "\n",
    "You now know **two toolkits:**\n",
    "\n",
    "Supervised → learn from labels (Modules 2–3).\n",
    "\n",
    "Unsupervised → explore structure without labels (Module 4).\n",
    "\n",
    "Real projects bounce between both: cluster customers 🡒 build a supervised model to predict cluster membership for new sign-ups, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff9f46-d9df-46a8-9cdb-52a3c194fef5",
   "metadata": {},
   "source": [
    "# Module 5 – Model Evaluation & Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56feaed3-3a56-4989-9e57-9031f777f464",
   "metadata": {},
   "source": [
    "### 1. Why this matters\n",
    "\n",
    "A model’s default settings are rarely optimal.\n",
    "    \n",
    "If you don’t measure properly you’ll:\n",
    "\n",
    "- Over-estimate performance (data leakage, cherry-picked split).\n",
    "\n",
    "- Waste hours tuning the wrong thing (e.g. maximising accuracy when the business cares about recall).\n",
    "\n",
    "Module 5 gives you the repeatable recipe:\n",
    "\n",
    "split ➜ cross-validate ➜ tune ➜ lock the test-set ➜ report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458934e-1e0c-4ea2-957a-2c1cfa2e5900",
   "metadata": {},
   "source": [
    "### 2. Cross-validation (CV) essentials\n",
    "\n",
    "| Strategy              | When to use                                                  | `sklearn` splitter | Typical folds |\n",
    "| --------------------- | ------------------------------------------------------------ | ------------------ | ------------- |\n",
    "| **k-Fold**            | i.i.d. tabular data                                          | `KFold` (default)  | 5 or 10       |\n",
    "| **Stratified k-Fold** | classification with imbalanced classes                       | `StratifiedKFold`  | 5 or 10       |\n",
    "| **Group k-Fold**      | samples grouped by user / product, no leakage between groups | `GroupKFold`       | #groups       |\n",
    "| **TimeSeriesSplit**   | chronological data                                           | `TimeSeriesSplit`  | 3–5           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf9aa4-ef0b-443c-a1e2-d8f1aa12c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scores = cross_val_score(model, X, y,\n",
    "                         cv=cv,\n",
    "                         scoring=\"roc_auc\")\n",
    "print(\"AUC per fold:\", scores.round(3),\n",
    "      \"mean:\", scores.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235994dc-4685-407b-90b8-752ddf890623",
   "metadata": {},
   "source": [
    "**Rule of thumb:** if std. dev > 0.02 on AUC/Accuracy, expect unstable generalisation → get more data or simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0d066-ab54-48af-860c-9bb1f54d7c4d",
   "metadata": {},
   "source": [
    "### 3. Data-leakage killers: Pipelines\n",
    "\n",
    "Without a pipeline you risk “peeking” at validation data during scaling / imputation.\n",
    "\n",
    "Pipeline & ColumnTransformer ensure every CV fold runs:\n",
    "\n",
    "- Fit transforms on train-fold only\n",
    "\n",
    "- Transform both train & val folds\n",
    "\n",
    "- Fit model on transformed train-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08090d1-5017-4f4f-9e7d-a23279a4396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),          # from Module 3\n",
    "    (\"model\", RandomForestClassifier(n_estimators=300,\n",
    "                                     max_depth=None,\n",
    "                                     random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba736809-f1dd-436c-8b0b-a65b06f430c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "You’ll pass this pipe straight into CV or grid-search—no extra work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b585b-8d8f-46e1-8633-c4e5c65cd25d",
   "metadata": {},
   "source": [
    "### 4. Hyper-parameter search methods\n",
    "\n",
    "| Method                    | `sklearn` class                     | Typical when…                                    | Pros                                 | Cons                                 |\n",
    "| ------------------------- | ----------------------------------- | ------------------------------------------------ | ------------------------------------ | ------------------------------------ |\n",
    "| **GridSearch**            | `GridSearchCV`                      | you have ≤ 4 params × a few values each          | exhaustive                           | combinatorial blow-up                |\n",
    "| **RandomSearch**          | `RandomizedSearchCV`                | bigger spaces, limited budget                    | explores wide range; early good hits | may miss small sweet-spots           |\n",
    "| **Bayesian / Sequential** | `skopt.BayesSearchCV` or **Optuna** | medium-large spaces, want fewer runs than random | learns from past trials              | extra dependency, slightly more code |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8284a-9f4f-4a2b-b292-acf29f15b311",
   "metadata": {},
   "source": [
    "#### 4.1 GridSearch example (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2df164-06b7-4bb3-845a-a66d1c6c2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [100, 300, 500],\n",
    "    \"model__max_depth\": [None, 8, 16],\n",
    "    \"model__max_features\": [\"sqrt\", 0.5, 0.8]\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(pipe,\n",
    "                   param_grid=param_grid,\n",
    "                   cv=5,\n",
    "                   scoring=\"roc_auc\",\n",
    "                   n_jobs=-1)\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best AUC:\", gcv.best_score_.round(3))\n",
    "print(\"Best params:\", gcv.best_params_)\n",
    "best_model = gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d412b-aaa4-47e0-b049-0f32a43520c0",
   "metadata": {},
   "source": [
    "**model__ prefix drills down into the Pipeline step called model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f4eb8-e776-41b5-b336-4c0d529df21a",
   "metadata": {},
   "source": [
    "#### 4.2 RandomisedSearch example (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de107c8-ac62-458d-a11c-455f9029f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 800),\n",
    "    \"model__max_depth\":    randint(3, 10),\n",
    "    \"model__learning_rate\": uniform(0.01, 0.2),\n",
    "    \"model__subsample\":    uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe,\n",
    "                          param_distributions=param_dist,\n",
    "                          n_iter=40,\n",
    "                          cv=5,\n",
    "                          scoring=\"f1\",\n",
    "                          random_state=0,\n",
    "                          n_jobs=-1)\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd208b7-5960-44d3-8007-68b25d6cf0c5",
   "metadata": {},
   "source": [
    "### 5. Nested CV – the gold-standard\n",
    "\n",
    "When the dataset is small and you need an honest performance claim:\n",
    "\n",
    "- Inner loop: tune hyper-params on GridSearchCV.\n",
    "\n",
    "- Outer loop: estimate generalisation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442cac-d417-4402-9cea-d8d9cab71736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "outer_scores = cross_val_score(rscv, X, y, cv=5, scoring=\"f1\")\n",
    "print(\"Nested CV F1:\", outer_scores.mean().round(3), \"±\", outer_scores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde18c3-0255-4a8b-9366-c8f9f7a8213e",
   "metadata": {},
   "source": [
    "**Time-consuming, but keeps the test-set pristine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23271fd2-db7b-4107-885c-b763d83c30c8",
   "metadata": {},
   "source": [
    "### 6. Final lock-down & model card\n",
    "\n",
    "Refit best model on train + val (all data except test).\n",
    "\n",
    "Evaluate once on test-set → final report.\n",
    "\n",
    "**Document in a model card**\n",
    "\n",
    "data source & time-range\n",
    "\n",
    "metrics (+ confidence intervals)\n",
    "\n",
    "fairness checks (e.g. by gender/region)\n",
    "\n",
    "limitations & caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d5ac8-d363-4719-8237-f4d77fa0e22a",
   "metadata": {},
   "source": [
    "### 7. Mini-Project – Tune & Benchmark\n",
    "\n",
    "**Choose one**\n",
    "\n",
    "Titanic survival (classification) – continue from Module 3\n",
    "\n",
    "California housing (regression) – sklearn.datasets.fetch_california_housing\n",
    "\n",
    "Your own CSV\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "Split train / test (80/20).\n",
    "\n",
    "Build a Pipeline with preprocessing + base model.\n",
    "\n",
    "RandomisedSearchCV over ≥ 4 hyper-params, cv=5, n_iter=40.\n",
    "\n",
    "Record gcv.best_params_ & best_score_.\n",
    "\n",
    "Refit on train + val, test once, print confusion matrix or RMSE.\n",
    "\n",
    "Optional: Nested CV – compare outer CV mean to test score.\n",
    "\n",
    "Summarise in 5 lines: “Model, tuned params, train/val CV metric, test metric, key insight”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f624ed-fd3b-4752-8f71-00090d046e4f",
   "metadata": {},
   "source": [
    "### 8. Coming up\n",
    "\n",
    "You now have the machinery to build reliable models.\n",
    "Next module we’ll dive into Feature Engineering & Interpretability:\n",
    "\n",
    "missing-value tricks, target encoding\n",
    "\n",
    "permutation importance, SHAP plots\n",
    "\n",
    "lifting your metric by ≥ 10 % with smarter features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874dc41-cffc-41ae-a85e-e7943f6a1837",
   "metadata": {},
   "source": [
    "# Module 6 – Feature Engineering & Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea1671-0708-4e07-ad64-b2dd1d78bd3e",
   "metadata": {},
   "source": [
    "### 1. Why features beat fancy algorithms\n",
    "\n",
    "A mediocre model fed useful features usually outperforms a state-of-the-art model fed raw data.\n",
    "\n",
    "Feature work also helps you understand the problem domain, setting you up for interpretability and fairness checks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d23a84-36bf-4dbc-b5f7-f7c06aae472e",
   "metadata": {},
   "source": [
    "### 2. Systematic Feature-Engineering Checklist\n",
    "\n",
    "| Category                               | Typical tricks                                                                                               | scikit-learn / Python tools                          |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------- |\n",
    "| **Numerical**                          | • log / square-root transforms for skewed data  <br>• binning into quantiles (captures non-linear jumps)     | `FunctionTransformer`, `KBinsDiscretizer`            |\n",
    "| **Dates & times**                      | • extract year, month, day-of-week, hour  <br>• cyclic encoding for hour-of-day / day-of-week (`sin`, `cos`) | `FeatureHasher` or a custom `FunctionTransformer`    |\n",
    "| **Categorical**                        | • One-hot encoding (baseline)  <br>• Frequency or **target encoding** for high-cardinality columns           | `OneHotEncoder`, `TargetEncoder` (category_encoders) |\n",
    "| **Text**                               | • TF-IDF, n-grams ≤ 3  <br>• Sentence embeddings (e.g. Sentence-BERT) if you need context                    | `TfidfVectorizer`, `sentence-transformers`           |\n",
    "| **Images**                             | • Pre-trained CNN embeddings (ResNet, EfficientNet…)                                                         | PyTorch/TensorFlow Hub                               |\n",
    "| **Interaction / polynomial**           | • products & ratios (price / income)  <br>• polynomial features of degree 2 or 3                             | `PolynomialFeatures`                                 |\n",
    "| **Aggregations** (temporal or grouped) | • mean / count per user, rolling 7-day sum                                                                   | pandas `groupby`, `rolling`                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3778c28-a698-4f80-a644-51a36c8d2421",
   "metadata": {},
   "source": [
    "**Rule of thumb:** engineer with domain logic first, then try automated methods (polynomial, feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fb84d-b9ca-4ba6-a502-26fb592da906",
   "metadata": {},
   "source": [
    "### 3. Pipelines for Safe Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c323a1-6101-424e-9f0e-225f61791de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing  import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.compose        import ColumnTransformer\n",
    "from sklearn.pipeline       import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Example: house prices\n",
    "date_feat = FunctionTransformer(\n",
    "        lambda s: np.c_[s.dt.year, s.dt.month, s.dt.dayofweek],\n",
    "        feature_names_out=lambda _, f: [\"year\", \"month\", \"dow\"])\n",
    "\n",
    "num_cols  = [\"sqft\", \"beds\", \"baths\", \"price_per_sqft\"]\n",
    "cat_cols  = [\"city\", \"home_type\"]\n",
    "date_col  = [\"sold_date\"]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\",  Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False))\n",
    "    ]), num_cols),\n",
    "    (\"cat\",  OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"date\", date_feat, date_col)\n",
    "])\n",
    "pipe = Pipeline([(\"prep\", preprocess),\n",
    "                 (\"model\", GradientBoostingRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa218c2-cb56-49a6-93fb-a7eb917b58e2",
   "metadata": {},
   "source": [
    "This guarantees that every fold in cross-validation sees transformations fitted only on its training slice—no leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405faa9-b4c2-4247-8069-f4084d54e0c8",
   "metadata": {},
   "source": [
    "### 4. Feature Selection & Importance\n",
    "\n",
    "#### 4.1 Filter Methods (fast, model-agnostic)\n",
    "\n",
    "Variance Threshold – drop near-constant columns.\n",
    "\n",
    "Univariate tests – e.g. SelectKBest with chi-square/F-score.\n",
    "\n",
    "#### 4.2 Embedded in the Model\n",
    "\n",
    "L1-regularised Logistic / Linear Regression → many coefficients driven to zero.\n",
    "\n",
    "Tree-based models → feature_importances_.\n",
    "\n",
    "#### 4.3 Wrapper Methods\n",
    "\n",
    "RFE / RFECV (Recursive Feature Elimination) – iteratively drop the weakest features according to an estimator until performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caa148-6f3f-4cac-b6d7-4e2413d1bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "selector = RFECV(pipe, step=1, cv=5, scoring=\"roc_auc\", n_jobs=-1)\n",
    "selector.fit(X, y)\n",
    "print(\"Optimal number of features:\", selector.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54884c6-8184-4de2-8e0c-83d2e4265018",
   "metadata": {},
   "source": [
    "### 5. Interpretability: looking inside black boxes\n",
    "\n",
    "#### 5.1 Global Explanations\n",
    "\n",
    "| Technique                                 | Works with                     | What you see                                    |\n",
    "| ----------------------------------------- | ------------------------------ | ----------------------------------------------- |\n",
    "| **Permutation Importance**                | any model                      | metric drop when a column’s values are shuffled |\n",
    "| **Mean Decrease in Impurity** (built-ins) | tree ensembles                 | native `feature_importances_` plot              |\n",
    "| **Partial Dependence (PDP)**              | any model (slower on big data) | curve of ŷ vs. a feature while averaging others |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570184cf-4bd9-448f-ad5a-410c21e5fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "result = permutation_importance(pipe, X_val, y_val, scoring=\"f1\", n_repeats=20)\n",
    "importances = pd.Series(result.importances_mean, index=pipe[\"prep\"].get_feature_names_out())\n",
    "importances.sort_values(ascending=False).head(10).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970edac0-769c-4647-ad85-3d4ec503da61",
   "metadata": {},
   "source": [
    "#### 5.2 Local Explanations\n",
    "\n",
    "| Technique                        | Library | Good for                                                 |\n",
    "| -------------------------------- | ------- | -------------------------------------------------------- |\n",
    "| **LIME**                         | `lime`  | single prediction “which features nudged this decision?” |\n",
    "| **SHAP** (TreeSHAP / KernelSHAP) | `shap`  | both global & local; consistent attribution              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1db6c-adc1-4733-a95d-0006ca7066c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(best_model[\"model\"])\n",
    "shap_values = explainer.shap_values(best_model[\"prep\"].transform(X_val.iloc[:200]))\n",
    "shap.summary_plot(shap_values, feature_names=pipe[\"prep\"].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e60a02-75ed-4b8d-a87d-0733988b41c4",
   "metadata": {},
   "source": [
    "**(For XGBoost/CatBoost/LightGBM, TreeSHAP runs fast; for other models use KernelSHAP on a sample.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dad2f-2e89-460f-aba5-3efacd589562",
   "metadata": {},
   "source": [
    "### 6. Mini-Project – Boost Your Model by ≥ 10 %\n",
    "\n",
    "Pick a previous supervised task (Titanic, housing, your own).\n",
    "\n",
    "Baseline: score from Module 5 after tuning.\n",
    "\n",
    "**Engineer at least 3 new features, e.g.**\n",
    "\n",
    "- log-transform skewed numerics\n",
    "\n",
    "- age buckets (cut into generations)\n",
    "\n",
    "- interaction term (price / sqft)\n",
    "\n",
    "Retrain (same CV & model).\n",
    "\n",
    "Compare CV metric – aim for ≥ 10 % relative improvement.\n",
    "\n",
    "**Explain**\n",
    "\n",
    "- Plot top-10 permutation importances.\n",
    "\n",
    "- Generate one SHAP force plot for an interesting single prediction (e.g. a mis-classified passenger).\n",
    "\n",
    "Write 5 bullet points summarising which features moved the needle and any surprising insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c817f-0cbd-42de-a1f6-6ad0d2b9a655",
   "metadata": {},
   "source": [
    "# Module 7 – Neural Networks & Deep Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757fa2f-87b6-4ce2-ab98-98f1e1d55214",
   "metadata": {},
   "source": [
    "### 0. Pick your framework & install\n",
    "\n",
    "| Choice                 | Why pick it                                                                | Stable version (May 2025)                   | Quick install                                                                                       |\n",
    "| ---------------------- | -------------------------------------------------------------------------- | ------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **PyTorch**            | Python-first, flexible training loops, huge research community             | **2.7.0** released Apr 23 2025 ([PyPI][1])  | `bash pip install torch torchvision torchaudio` *(add the GPU wheel URL if you have CUDA 12+)*      |\n",
    "| **TensorFlow / Keras** | Easiest “model-build-compile-fit”, production extras (TF-Lite, TF-Serving) | **2.19.0** released Mar 12 2025 ([PyPI][2]) | `bash pip install tensorflow` *(CPU & GPU wheels auto-select; Apple Silicon gets NEON/Metal accel)* |\n",
    "\n",
    "[1]: https://pypi.org/project/torch/?utm_source=chatgpt.com \"torch - PyPI\"\n",
    "[2]: https://pypi.org/project/tensorflow/?utm_source=chatgpt.com \"TensorFlow - PyPI\"\n",
    "\n",
    "Both sites keep a single “Stable” tab that always shows the latest compatible wheel and CUDA/cuDNN matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3e62c-4447-49fa-93c3-abff903ffbe6",
   "metadata": {},
   "source": [
    "### 1. Building blocks you must know\n",
    "\n",
    "| Concept              | 1-sentence memory hook                                                                |\n",
    "| -------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **Neuron**           | Weighted sum → non-linear activation (ReLU, GELU, Sigmoid…).                          |\n",
    "| **Layer / MLP**      | Stack of neurons; depth lets you model non-linear relations.                          |\n",
    "| **Forward pass**     | Compute ŷ for a batch.                                                                |\n",
    "| **Loss**             | Scalar that says “how wrong?” (cross-entropy for classification, MSE for regression). |\n",
    "| **Back-propagation** | Auto-diff calculates ∂Loss/∂Weights; optimiser (SGD, Adam, Lion) updates weights.     |\n",
    "| **Epoch**            | One full sweep over training data.                                                    |\n",
    "| **Batch size**       | Rows processed before an optimiser step—trade-off RAM vs noisy gradients.             |\n",
    "\n",
    "Once you can recite that loop—forward → loss → backward → step—you understand every neural-net paper diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257abf7-310f-4168-b0f4-2a2e6382667e",
   "metadata": {},
   "source": [
    "### 2. Key architectures on one slide\n",
    "\n",
    "| Family               | Typical problem                   | Core idea                                                      |\n",
    "| -------------------- | --------------------------------- | -------------------------------------------------------------- |\n",
    "| **MLP (Dense)**      | Tabular numbers, basic NLP        | Fully-connected layers; good baseline.                         |\n",
    "| **CNN**              | Images, audio spectrograms        | Convolution filters share weights → spatial pattern detection. |\n",
    "| **RNN / LSTM / GRU** | Time-series, small-scale language | Hidden state carries information along sequence.               |\n",
    "| **Transformer**      | Large-scale NLP, vision (ViT)     | Self-attention learns pairwise token relations in parallel.    |\n",
    "\n",
    "You’ll implement an MLP today; CNN and Transformer quick-starts are optional stretch goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c7729-fd05-4f49-b993-cda54d6639e0",
   "metadata": {},
   "source": [
    "### 3. PyTorch Crash-course (40 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63e179-fe2c-4ef2-aba3-8a2a71f5b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1. Data pipeline (MNIST 28×28 → tensor [0,1])\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True,\n",
    "                          transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "# 2. Model (simple 2-hidden-layer MLP)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 64),   nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.seq(x)\n",
    "\n",
    "model = Net().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. Loss & optimiser\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# 4. Training loop (3 epochs)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(model.device), y.to(model.device)\n",
    "        optimiser.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"Epoch {epoch+1}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1adb2-1675-4cc4-a5b2-68b2e6422701",
   "metadata": {},
   "source": [
    "**Add a torch.no_grad() validation block after each epoch and hook in TensorBoard:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519f7ad-0f2d-4de6-9100-c22027f0a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorboard; tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df56a-e615-4f70-954e-f071b94d9103",
   "metadata": {},
   "source": [
    "### 4. Keras (TensorFlow) same idea in 12 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f86cdd-03e7-4fc3-842c-2e3698932a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_val = x_train/255.0, x_val/255.0\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dense(64,  activation=\"relu\"),\n",
    "    layers.Dense(10,  activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574862f-0a03-4f80-b515-074d6a845884",
   "metadata": {},
   "source": [
    "Keras writes TensorBoard logs automatically when you add callbacks=[tf.keras.callbacks.TensorBoard()]. GPU is used by default if available (NVIDIA, Apple-Metal, ROCm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7fac0-a7bb-40e1-a7d5-1b7ea7a5c284",
   "metadata": {},
   "source": [
    "### 5. From CSVs & images to tensors\n",
    "\n",
    "**Tabular:** pandas → .values.astype(np.float32) → torch.tensor or tf.convert_to_tensor.\n",
    "\n",
    "**Images:** torchvision.datasets.ImageFolder or tf.keras.utils.image_dataset_from_directory.\n",
    "\n",
    "**Text:** Hugging Face datasets.load_dataset(\"imdb\") → AutoTokenizer → tensors.\n",
    "\n",
    "Batching and shuffling happen in the DataLoader (PyTorch) or tf.data.Dataset (TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0c906-0792-4aa2-91a3-6c574c84dfc6",
   "metadata": {},
   "source": [
    "### 6. Monitoring & preventing over-fit\n",
    "\n",
    "| Tool                       | Works with                                              | Shows                                      |\n",
    "| -------------------------- | ------------------------------------------------------- | ------------------------------------------ |\n",
    "| **TensorBoard**            | both TF & PyTorch (`torch.utils.tensorboard`)           | loss/metric curves, GPU stats, embeddings. |\n",
    "| **EarlyStopping callback** | Keras `tf.keras.callbacks.EarlyStopping`                | stop when val-loss stagnates.              |\n",
    "| **LR schedulers**          | `torch.optim.lr_scheduler` or Keras `ReduceLROnPlateau` | auto-tune learning rate.                   |\n",
    "| **Regularisers**           | Dropout, weight-decay (L2), data-augmentation           | baked into layers / optimiser.             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23452e-a276-4264-9c26-e99f44e6f34e",
   "metadata": {},
   "source": [
    "### 7. Mini-projects (pick one)\n",
    "\n",
    "**🖼️ Image – CIFAR-10 with Keras**\n",
    "\n",
    "- tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "- Normalise to [0,1], one-hot labels.\n",
    "\n",
    "- Replace MLP with a small CNN: Conv2D → MaxPool → Flatten → Dense.\n",
    "\n",
    "- Train 15 epochs, watch val-accuracy aim > 70 %.\n",
    "\n",
    "- Plot 5 mis-classified images; use Grad-CAM (tf.keras.applications.vgg16.preprocess_input) for insight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5f6dc-5df5-4ef7-bfe2-2dfe52d963ed",
   "metadata": {},
   "source": [
    "**✍️ Text – IMDb sentiment with Transformers (PyTorch)**\n",
    "\n",
    "- pip install transformers datasets accelerate\n",
    "\n",
    "- Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbcf08c-1331-4d80-b7be-d82899c0badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863517a-1ad0-4575-b86d-c5d69e17adf2",
   "metadata": {},
   "source": [
    "- AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") → encode.\n",
    "\n",
    "- AutoModelForSequenceClassification → Trainer API.\n",
    "\n",
    "- Evaluate accuracy on test split; aim > 90 %.\n",
    "\n",
    "- Use transformers.Interpretation or captum for token-level importance on one review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f82a33-beb4-4a08-9d29-727df9d17bd4",
   "metadata": {},
   "source": [
    "### 8. Where we’re headed\n",
    "\n",
    "You now have three engines:\n",
    "\n",
    "- Classic ML (Modules 2–6)\n",
    "\n",
    "- Deep-learning basics (this module)\n",
    "\n",
    "- Everything ready to deploy (Module 8 – MLOps & serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0ec36-ec96-4455-abd8-f97d64a2df0a",
   "metadata": {},
   "source": [
    "# Module 8 – Model Deployment & MLOps Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a7f6d-582a-4051-af73-b23b4a3bfbf2",
   "metadata": {},
   "source": [
    "### 0. Why shipping your model is a skill of its own\n",
    "\n",
    "A model that lives only in a notebook has zero real-world value. Deployment turns it into a service that other code—or people—can actually call. MLOps then keeps that service fast, reliable and trustworthy over time (versioning, monitoring, rollback)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a025fbc-762f-4f1d-b7de-c204bf42b23a",
   "metadata": {},
   "source": [
    "### 1. Save & version the trained model\n",
    "\n",
    "| Format              | Good for                                  | How                                                                                   |\n",
    "| ------------------- | ----------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **joblib / pickle** | small scikit-learn or XGBoost objects     | `joblib.dump(pipe, \"model.joblib\")`                                                   |\n",
    "| **ONNX**            | language-agnostic, runs in C#/Java/mobile | `skl2onnx.convert_sklearn(pipe)`                                                      |\n",
    "| **MLflow**          | experiment tracking + artefact registry   | `mlflow.sklearn.log_model(pipe, \"model\")` (MLflow 2.22 out Apr 24 2025) ([MLflow][1]) |\n",
    "\n",
    "[1]: https://mlflow.org/?utm_source=chatgpt.com \"MLflow | MLflow\"\n",
    "\n",
    "**Rule:** store every artefact (model file, scaler, config) in a versioned directory named after the Git commit or MLflow run ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023583a-1723-4e0e-b32a-c1d347cc433a",
   "metadata": {},
   "source": [
    "### 2. Pick a serving stack\n",
    "\n",
    "| Path                                       | When to choose it                                        | Key tool/version                        |\n",
    "| ------------------------------------------ | -------------------------------------------------------- | --------------------------------------- |\n",
    "| **Pure FastAPI** REST                      | lightweight, Python-only microservice                    | FastAPI 0.115 (Mar 23 2025) ([PyPI][1]) |\n",
    "| **Flask** REST                             | smallest footprint, simple scripts                       | Flask 3.x                               |\n",
    "| **BentoML**                                | turnkey “model → Docker/OCI image” pipeline, multi-model | BentoML 1.4 (Feb 2025) ([BentoML][2])   |\n",
    "| **Serverless** (AWS Lambda, GCP Cloud Run) | bursty traffic, pay-per-use                              | Container image or zip-deploy           |\n",
    "| **Spark/Databricks MLflow**                | large-scale batch scoring                                | MLflow model registry                   |\n",
    "\n",
    "[1]: https://pypi.org/project/fastapi/?utm_source=chatgpt.com \"fastapi - PyPI\"\n",
    "[2]: https://www.bentoml.com/blog/announcing-bentoml-1-4?utm_source=chatgpt.com \"Announcing BentoML 1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fd45c-03ec-4c0a-b577-4d9f2f51f2e8",
   "metadata": {},
   "source": [
    "### 3. FastAPI service in 25 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65ff71-f595-488a-b6b0-2b09b89e3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "class Passenger(BaseModel):          # tailor to your feature schema\n",
    "    pclass:int; sex:str; age:float; fare:float\n",
    "\n",
    "app = FastAPI(title=\"Titanic Survival API\")\n",
    "model = joblib.load(\"model.joblib\")   # trained Pipeline from Module 5/6\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(p: Passenger):\n",
    "    import pandas as pd\n",
    "    X = pd.DataFrame([p.dict()])\n",
    "    proba = model.predict_proba(X)[0,1]\n",
    "    return {\"survival_probability\": round(float(proba), 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939e0c3-de28-48f3-8af2-372f638af8af",
   "metadata": {},
   "source": [
    "#### **Local test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f3112-5ca8-49cc-b8d1-fb56f7de7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install deps\n",
    "pip install fastapi uvicorn[standard] joblib pandas\n",
    "\n",
    "# 2. Run dev server\n",
    "uvicorn app:app --reload\n",
    "\n",
    "# 3. Call\n",
    "curl -X POST http://127.0.0.1:8000/predict \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"pclass\":2,\"sex\":\"female\",\"age\":28,\"fare\":28.0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052dcea2-4410-4d50-afd3-6f8ab6382091",
   "metadata": {},
   "source": [
    "**Swagger UI auto-appears at /docs—great for QA without writing a line of front-end code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7563d-733a-4d95-bf7c-c4901e1b95fb",
   "metadata": {},
   "source": [
    "### 4. Containerise with Docker 25\n",
    "\n",
    "Create a plain text file named Dockerfile in the project root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dfd96-7b5e-4cd9-9612-22c46935e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax=docker/dockerfile:1\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN  pip install -r requirements.txt            # scikit-learn, fastapi, uvicorn,...\n",
    "\n",
    "COPY . .\n",
    "ENV PORT=80\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce86d5-7b49-4263-8296-7d9837c8024e",
   "metadata": {},
   "source": [
    "#### **Bash**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06dc21-d75f-4ae8-8896-e3067fbcff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & run\n",
    "docker build -t titanic-api:1.0 .\n",
    "docker run -d -p 80:80 titanic-api:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d5fb0-6132-4ef6-9dbd-6598c278a21b",
   "metadata": {},
   "source": [
    "Docker Engine 25 reached stable GA earlier this year; stick to 25.x for latest security patches and BuildKit improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95254d-fbb9-43e5-b708-2d93b288137c",
   "metadata": {},
   "source": [
    "### 5. Continuous Delivery (CD) with GitHub Actions\n",
    "\n",
    "(one-file minimal example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2ab88-c983-41fd-a720-57be9c610210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .github/workflows/deploy.yml\n",
    "name: CI-CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build-and-push:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - name: Set tag\n",
    "        id: vars\n",
    "        run: echo \"TAG=${GITHUB_SHA::7}\" >> $GITHUB_ENV\n",
    "      - name: Build\n",
    "        uses: docker/build-push-action@v5\n",
    "        with:\n",
    "          push: true\n",
    "          tags: ghcr.io/${{ github.repository }}:${{ env.TAG }}\n",
    "          context: .\n",
    "      - name: Deploy to staging\n",
    "        run: |\n",
    "          curl -X POST $STAGING_TRIGGER_URL \\\n",
    "               -H \"Authorization: Bearer $TOKEN\" \\\n",
    "               -d \"{\\\"image\\\":\\\"ghcr.io/$GITHUB_REPOSITORY:${{ env.TAG }}\\\"}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3026ba-eac6-43cf-a2fb-9a23ab898b80",
   "metadata": {},
   "source": [
    "Replace $STAGING_TRIGGER_URL with your cloud’s image-update endpoint (Fly.io, Render, ECS, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d870d3a-c3c8-4c57-ac1b-e04566512899",
   "metadata": {},
   "source": [
    "### 6. Monitoring & Drift Detection\n",
    "\n",
    "| Concern                      | Open-source helper                                     |\n",
    "| ---------------------------- | ------------------------------------------------------ |\n",
    "| **Service uptime / latency** | Prometheus + Grafana                                   |\n",
    "| **Model & data drift**       | Evidently (0.5 + drift dashboards) ([Evidently AI][1]) |\n",
    "| **Experiment lineage**       | MLflow tracking server                                 |\n",
    "| **Alerting**                 | Grafana alerts ➜ Slack / Teams                         |\n",
    "\n",
    "[1]: https://www.evidentlyai.com/?utm_source=chatgpt.com \"Evidently AI - AI Testing & LLM Evaluation Platform\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c37348-0c4f-4975-ad67-4a2c56d1d666",
   "metadata": {},
   "source": [
    "#### **Example: Evidently quick-check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097c107-1944-45d1-b5ca-629fc7f0729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftPreset, RegressionPerformancePreset\n",
    "\n",
    "ref = pd.read_parquet(\"2024-12-reference.parquet\")\n",
    "cur = pd.read_parquet(\"2025-05-current.parquet\")\n",
    "\n",
    "rpt = Report(metrics=[DataDriftPreset(), RegressionPerformancePreset()])\n",
    "rpt.run(reference_data=ref, current_data=cur)\n",
    "rpt.save_html(\"drift.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56860bc-6755-434d-be80-b7d8f6c49053",
   "metadata": {},
   "outputs": [],
   "source": [
    "Serve drift.html behind Basic-Auth or drop it in S3 for your data team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae15a27-4132-4413-af1c-7a997593ad0d",
   "metadata": {},
   "source": [
    "### 7. Mini-project – Put Your Model on the Internet 🌐\n",
    "\n",
    "- Pick any trained model from earlier modules.\n",
    "\n",
    "- Serialize → model.joblib.\n",
    "\n",
    "- Wrap with FastAPI (see §3).\n",
    "\n",
    "- Add requirements.txt (FastAPI, uvicorn, pandas, scikit-learn, joblib).\n",
    "\n",
    "- Containerise with Dockerfile (§4).\n",
    "\n",
    "- Run locally and hit /predict with curl or Postman.\n",
    "\n",
    "- (Stretch) Push to GitHub, enable GH Actions CD, deploy to free tier on Fly.io, Render or Railway.\n",
    "\n",
    "- Smoke-test one happy path and one error path (missing field).\n",
    "\n",
    "- Share: post your service URL or Docker Hub tag here—I’ll test and review security headers, latency, and suggest observability improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd1396-f67e-4947-bacb-e48ef53ac2de",
   "metadata": {},
   "source": [
    "### 8. Where we go from here\n",
    "\n",
    "You can now ship models, reproduce builds, and watch them in production—a huge milestone!\n",
    "\n",
    "The final module will cover Responsible AI & Fairness: bias audits, SHAP for fairness, model cards, and privacy basics.\n",
    "\n",
    "When your FastAPI endpoint returns a 200 OK (or you want help making it do so), drop “next” and we’ll finish our ML A-to-Z journey with ethics and responsible deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd466ae-9509-4161-9af0-b5f23c338a91",
   "metadata": {},
   "source": [
    "# Module 9 – Responsible AI: Ethics, Fairness, Privacy & Transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8410e7c-eb70-48e0-a6d0-c4c923408bab",
   "metadata": {},
   "source": [
    "### 1. Why this matters\n",
    "\n",
    "Even a perfectly tuned, high-accuracy model can harm people—by amplifying bias, leaking personal data or behaving unpredictably once it meets the real world. New laws (EU AI Act, FTC actions, ISO 42001) now make responsible-AI practices mandatory, not optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b8be9-d6f3-4804-9d24-2e7ee62d451d",
   "metadata": {},
   "source": [
    "### 2. The regulatory landscape you must track (2024-25)\n",
    "\n",
    "| Regulation / Standard                    | Scope & key duties                                                                                                                                                                                                                                                                                                                        | In force                                              |\n",
    "| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| **EU AI Act**                            | *Risk-based tiers:* minimal → limited → **high-risk** → prohibited. High-risk systems (e.g. hiring, credit, health) need a **fundamental-rights impact assessment**, quality management, transparency & human oversight. Foundation-model providers must publish energy use & test for systemic risks. ([Artificial Intelligence Act][1]) | Gradual: some rules mid-2025, full compliance by 2026 |\n",
    "| **NIST AI RMF 1.0**                      | Voluntary U.S. framework—IDENTIFY, GOVERN, MAP, MEASURE, MANAGE functions—to systematise trustworthy-AI risk control across the lifecycle. ([NIST][2], [NIST Technical Series Publications][3])                                                                                                                                           | Jan 2024                                              |\n",
    "| **ISO/IEC 42001**                        | First certifiable **AI management-system** standard (akin to ISO 27001 for security) covering ethics, transparency, continuous risk review. ([ISO][4], [KPMG][5])                                                                                                                                                                         | Dec 2024                                              |\n",
    "| **AI Safety Summits (Bletchley, Paris)** | Multilateral “Bletchley Declaration” commits 29 countries to share safety testing for frontier models. ([GOV.UK][6], [European Payments Council][7])                                                                                                                                                                                      | 2024                                                  |\n",
    "| **FTC enforcement**                      | U.S. regulator now fines vendors for deceptive or biased AI claims (e.g., IntelliVision settlement Jan 2025). ([Lathrop GPM][8], [Federal Trade Commission][9])                                                                                                                                                                           | Active                                                |\n",
    "\n",
    "[1]: https://artificialintelligenceact.eu/high-level-summary/?utm_source=chatgpt.com \"High-level summary of the AI Act | EU Artificial Intelligence Act\"\n",
    "[2]: https://www.nist.gov/itl/ai-risk-management-framework?utm_source=chatgpt.com \"AI Risk Management Framework | NIST\"\n",
    "[3]: https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf?utm_source=chatgpt.com \"[PDF] Artificial Intelligence Risk Management Framework (AI RMF 1.0)\"\n",
    "[4]: https://www.iso.org/standard/81230.html?utm_source=chatgpt.com \"ISO/IEC 42001:2023 - AI management systems\"\n",
    "[5]: https://kpmg.com/ch/en/insights/artificial-intelligence/iso-iec-42001.html?utm_source=chatgpt.com \"ISO/IEC 42001: The latest AI management system standard\"\n",
    "[6]: https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023?utm_source=chatgpt.com \"The Bletchley Declaration by Countries Attending the AI Safety ...\"\n",
    "[7]: https://epc.eu/publication/The-Paris-Summit-Au-Revoir-global-AI-Safety-61ea68/?utm_source=chatgpt.com \"The Paris Summit: Au Revoir, global AI Safety?\"\n",
    "[8]: https://www.lathropgpm.com/insights/transparency-and-ai-ftc-launches-enforcement-actions-against-businesses-promoting-deceptive-ai-product-claims/?utm_source=chatgpt.com \"Transparency and AI: FTC Launches Enforcement Actions Against ...\"\n",
    "[9]: https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2025/01/ai-risk-consumer-harm?utm_source=chatgpt.com \"AI and the Risk of Consumer Harm | Federal Trade Commission\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c43b39-5377-43cc-9eee-afd395173541",
   "metadata": {},
   "source": [
    "### 3. Fairness fundamentals\n",
    "\n",
    "| Concept                    | Quick definition                                                                                                 | Typical metric                                              |\n",
    "| -------------------------- | ---------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Sensitive attribute**    | Feature legally / ethically protected (sex, race, disability, age…)                                              | —                                                           |\n",
    "| **Group fairness**         | Outcome rates are comparable across sensitive groups                                                             | *Demographic parity*, *Equalised odds*, *Equal opportunity* |\n",
    "| **Individual fairness**    | “Similar individuals receive similar outcomes”                                                                   | Counter-factual fairness distance                           |\n",
    "| **Bias mitigation** stages | *Pre-processing* (re-weighing), *In-processing* (adversarial debias), *Post-processing* (threshold optimisation) | —                                                           |\n",
    "\n",
    "Trade-offs: perfect parity can lower overall accuracy; document the business-ethics decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d265f-1fb8-4c23-965a-5554998b0f96",
   "metadata": {},
   "source": [
    "### 4. Interpretability & transparency toolkit\n",
    "\n",
    "| Goal                  | Reality-check questions                         | Tools / libraries                                        |\n",
    "| --------------------- | ----------------------------------------------- | -------------------------------------------------------- |\n",
    "| **Global insight**    | “Which features drive decisions overall?”       | Permutation importance, SHAP summary plot                |\n",
    "| **Local explanation** | “Why did *this* person get denied?”             | SHAP force plot, LIME                                    |\n",
    "| **Robustness**        | “Does a small input tweak flip the prediction?” | Adversarial tests, sensitivity analysis                  |\n",
    "| **Documentation**     | “Can a non-tech auditor understand the system?” | *Model cards*, *datasheets for datasets*, *system cards* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d126aec-f4bf-4334-a06b-6b42d5aab343",
   "metadata": {},
   "source": [
    "### 5. Privacy & security check-list\n",
    "\n",
    "- Minimise data: drop columns you would not show on your dashboard.\n",
    "\n",
    "- Pseudonymise IDs, use k-anonymity + differential privacy when releasing stats.\n",
    "\n",
    "- Secure the supply chain: signed Docker images, dependency-pinning.\n",
    "\n",
    "- Red-team generative models: jailbreak prompts, prompt-injection, data-leak tests (EU AI Act Article 53)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f0105-2ecf-4bc1-a0e4-6bad69876fe0",
   "metadata": {},
   "source": [
    "### 6. Hands-on mini-project – Fairness audit & mitigation\n",
    "\n",
    "#### 6.1 Dataset\n",
    "\n",
    "Adult Income (a.k.a. “Census Income”, 48 k rows, predict income > $50 k). Contains sex and race attributes—classic for bias demos.\n",
    "\n",
    "#### 6.2 Objectives\n",
    "\n",
    "- Baseline: Train a tuned Logistic Regression or Random Forest (Module 5 pipeline).\n",
    "\n",
    "- Audit: Compute accuracy plus\n",
    "\n",
    "Demographic parity difference (DPD) & Equalised odds difference (EOD) for sex and race.\n",
    "\n",
    "- Mitigate: Use Fairlearn’s ExponentiatedGradient or GridSearch to reduce both gaps while keeping ≥ 95 % of baseline accuracy.\n",
    "\n",
    "- Report: One chart (Fairlearn dashboard) + 5 bullets answering:\n",
    "\n",
    "Which metric improved? • What trade-off did you accept? • Any residual bias? • How will you monitor in prod? • Next mitigation ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c175a-11e9-446a-88b2-d50b1595bd5b",
   "metadata": {},
   "source": [
    "#### 6.3 Code pseudostarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee2687-e3a8-4170-94ed-a3fed4b0f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fairlearn==0.11.0\n",
    "\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, \\\n",
    "                               equalized_odds_difference, accuracy_score\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"adult.csv\")                       # load data\n",
    "y = (df[\"income\"] == \">50K\").astype(int)\n",
    "X = df.drop(columns=[\"income\"])\n",
    "\n",
    "sens = df[\"sex\"]                                    # try also 'race'\n",
    "X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
    "        X, y, sens, stratify=y, test_size=0.2, random_state=0)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), X.select_dtypes(\"number\").columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                X.select_dtypes(\"object\").columns)\n",
    "])\n",
    "\n",
    "base = Pipeline([(\"prep\", pre),\n",
    "                 (\"clf\",  LogisticRegression(max_iter=2000))])\n",
    "base.fit(X_tr, y_tr)\n",
    "\n",
    "y_hat = base.predict(X_te)\n",
    "mf = MetricFrame(metrics=dict(acc=accuracy_score,\n",
    "                              dp=demographic_parity_difference,\n",
    "                              eo=equalized_odds_difference),\n",
    "                 y_true=y_te, y_pred=y_hat, sensitive_features=s_te)\n",
    "print(\"Baseline:\", mf.by_group)\n",
    "\n",
    "# Mitigation\n",
    "mitigator = ExponentiatedGradient(base, constraints=DemographicParity())\n",
    "mitigator.fit(X_tr, y_tr, sensitive_features=s_tr)\n",
    "y_hat_m = mitigator.predict(X_te)\n",
    "mf_m = mf._replace(y_pred=y_hat_m)     # re-run metrics\n",
    "print(\"Mitigated:\", mf_m.by_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec034c8-20d0-45f1-a4e5-64eb44e326b4",
   "metadata": {},
   "source": [
    "### 7. Production guard-rails\n",
    "\n",
    "| Stage            | Must-have guard-rail                                                                           |\n",
    "| ---------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Pre-launch**   | *Fairness report* signed-off by legal/ethics lead; attach to Model Card v1.                    |\n",
    "| **Daily batch**  | Drift dashboard (Evidently) auto-checks sensitive-attribute distributions; alert at 3 σ shift. |\n",
    "| **Realtime API** | Log inputs & outputs (hashed IDs) for traceability; sample 0.5 % for human review.             |\n",
    "| **Versioning**   | Tag model + data snapshot; keep older version live until new one passes A/B guard-rail.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad59963-1067-4f7b-8590-c02d7a42e351",
   "metadata": {},
   "source": [
    "### 8. Reflection & wrap-up\n",
    "\n",
    "- Deliver your audit notebook (or errors) and we’ll iterate on tougher mitigations (counter-factual, Causal ML).\n",
    "\n",
    "- Update your model card to include: intended use, out-of-scope use, ethical risks, fairness metrics, mitigations, contact for redress.\n",
    "\n",
    "- Re-train with privacy enhancements (e.g., 𝜖 ≤ 1 differential-privacy logistic regression) if data is sensitive.\n",
    "\n",
    "You’ve now completed the A → Z ML journey:\n",
    "\n",
    "Foundations → Classic ML → Unsupervised → Tuning → Feature-craft → Deep-learning → Deployment → Responsible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92197c-5436-4547-8148-96c880ec57a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
